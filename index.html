<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VISY-REVE creates continuous imaged trajectories from discrete datasets.">
  <meta name="keywords" content="VISY-REVE, VBN, View-Synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VISY-REVE: Real-Time View Synthesis for Vision-Based Navigation</title>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VISY-REVE: View Synthesis for Real-Time Validation of Vision-Based Navigation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://marius-ne.github.io/">Marius Neuhalfen</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              Jonathan Grzymisch<sup>2</sup>,</span>
            <span class="author-block">
              Manuel Sánchez-Gestido<sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>European Space Agency, ESTEC</span>
            <span class="author-block"><sup>2</sup>RWTH Aachen University</span>
            <span class="author-block"><sup>3</sup>École Centrale de Lille</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2507.02993"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/marius-ne/VISY-REVE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- LinkedIn Link. -->
              <span class="link-block">
                <a href="https://www.linkedin.com/in/marius-neuhalfen/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                        <i class="fab fa-linkedin"></i>
                  </span>
                  <span>LinkedIn</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">VISY-REVE</span> turns discrete datasets into responsive, continuous trajectories
        in real time. 
      </h2>
    </div>
  </div>
</section>


<section class="section is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This work introduces VISY-REVE: a novel pipeline to validate image processing algorithms for Vision-Based Navigation. Traditional validation methods such as synthetic rendering or robotic testbed acquisition suffer from difficult setup and slow runtime. Instead, we propose augmenting image datasets in real-time with synthesized views at novel poses. This approach creates continuous trajectories from sparse, pre-existing datasets in open or closed-loop. In addition, we introduce a new distance metric between camera poses, the Boresight Deviation Distance, which is better suited for view synthesis than existing metrics. Using it, a method for increasing the density of image datasets is developed.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">


    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Contributions</h2>

        <!-- Re-rendering. -->
        <div class="content has-text-justified">
          <p>
            Our <b>key contributions</b> are the introduction of a real-time view synthesis pipeline and
            alongside this a novel distance metric between poses that is highly predictive of the quality
            of synthesized image.
          </p>
        </div>
        <div class="content has-text-centered">
            <figure class="image" style="display: inline-block; width: 75%;">
            <img src="./static/images/illustrations_linux2.png" alt="Animation placeholder image">
            </figure>
        </div>
        <!--/ Re-rendering. -->

        <h3 class="title is-3">Applications</h2>
        <div class="content has-text-justified">
          <p>
            Our pipeline can be used to enable <b>closed-loop</b> testing from existing,
            <b>sparse</b> datasets, thereby avoiding time-consuming synthetic rendering or 
            tedious setup of closed-loop operations in a facility.
          </p>
          <p>
            It can also be used to <i>densify</i> datasets, that is, increasing their sampling
            density offline after they have been acquired / generated. This serves to fill
            "holes" that could not be sampled during the acqusition process due to computing
            or facility constraints. The synthesized samples can be chosen in a way to specifically
            fulfill certain quality requirements.
          </p>
        </div>

        <h3 class="title is-3">View Synthesis Methods</h2>
        <div class="content has-text-justified">
          <p>
            We use traditional, analytic view synthesis methods to transform source views of 
            an existing dataset into novel views. We propose two methods, one that favors fast
            computation while the other favors accurate synthesis.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <figure class="image" style="display: inline-block; width: 90%;">
              <img src="./static/images/transform2DPipeline_V4.png" alt="Animation placeholder image 1">
            </figure>
          </div>
          <div class="column has-text-centered">
            <figure class="image" style="display: inline-block; width: 90%;">
              <img src="./static/images/transform3DPipeline_V3.png" alt="Animation placeholder image 2">
            </figure>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            The novel distance metric that we introduce is called the <b>Boresight Deviation Distance</b>. 
            It has been designed to only consider the important degrees of freedom that most effect the 
            quality of synthesized views. This has two main advantages: 1. It enables choosing optimal nearest-neighbor
            views to perform view synthesis; 2. It enables creating more liberal performance models.
          </p>
          <p>
            Below you can see the intuition for this new metric as well as its 3D plot.
          </p>
        </div>
        <div class="columns is-centered">
        <!-- Left column: 60% width -->
        <div class="column is-three-fifths has-text-centered">
          <figure class="image" style="display:inline-block; width:90%;">
            <img src="./static/images/bddillustration.png" alt="Animation placeholder image 1">
          </figure>
        </div>

        <!-- Right column: 40% width -->
        <div class="column is-two-fifths has-text-centered">
          <figure class="image" style="display:inline-block; width:90%;">
            <img src="./static/images/BDD_3D_wo_BGV2.png" alt="Animation placeholder image 2">
          </figure>
        </div>
      </div>

      <h3 class="title is-3">Future Work</h2>
        <div class="content has-text-justified">
          <p>
            In the future we would like to expand our view synthesis methods to include more
            modern approaches such as NeRF or Gaussian Splatting. Both of these would allow 
            to synthesize not only geometric but also radiometric (i.e. lighting) changes
            associated to pose changes. This would prove especially useful for enhancing
            image datasets offline.
          </p>
        </div>

      <div class="content has-text-justified">
          <p>
            Feel free to see the paper for more information, thank you for reading!
          </p>
        </div>
      </div>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{neuhalfen2025enablingrobustrealtimeverification,
      title={Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis}, 
      author={Marius Neuhalfen and Jonathan Grzymisch and Manuel Sanchez-Gestido},
      year={2025},
      eprint={2507.02993},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.02993}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://www.arxiv.org/pdf/2507.02993">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/marius-ne/VISY-REVE" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is a fork of the NERFIES website, which is available at <a href="https://nerfies.github.io/">https://nerfies.github.io/</a> 
            with its source code public at <a href="https://github.com/nerfies/nerfies.github.io">https://github.com/nerfies/nerfies.github.io</a>.
            We kindly thank its authors for allowing us to use it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
